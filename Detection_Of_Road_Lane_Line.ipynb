{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Objective**"
      ],
      "metadata": {
        "id": "68Ob250ROl9P"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The objective of the road lane line detection project in data science is to develop a\n",
        "computer vision system that can accurately identify and localize lane lines on the road\n",
        "from input images or video streams. This system is crucial for various applications, such\n",
        "as autonomous vehicles, advanced driver-assistance systems (ADAS), and road safety\n",
        "analysis."
      ],
      "metadata": {
        "id": "LVuo-ViMOxvV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this project, I used Python and OpenCV to detect lane lines on the road. I developed a processing pipeline that works on a series of individual images, and applied the result to a video stream."
      ],
      "metadata": {
        "id": "HN7iRYqgRWoZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Loading test images**"
      ],
      "metadata": {
        "id": "UDKSKCo7RljH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We have 6 test images. We will write a function called list_images() that will show all the test images we're working on."
      ],
      "metadata": {
        "id": "pUuhXjA9Rr2g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Importing some useful packages\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.image as mpimg # This imports the mpimg module from matplotlib for working with images.\n",
        "import numpy as np\n",
        "import cv2 # This imports the OpenCV library, which is widely used for computer vision tasks such as image processing and manipulation.\n",
        "import os # os module for interacting with the operating system, which can be used for tasks like file handling and directory operations.\n",
        "import glob # This imports the glob module, which is used for file path expansion and searching\n",
        "from moviepy.editor import VideoFileClip # This imports the VideoFileClip class from the moviepy library, which is used for video editing and manipulation.\n",
        "\n",
        "%matplotlib inline"
      ],
      "metadata": {
        "id": "fsQNhDzROybI"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def list_images(images, cols = 2, rows = 5, cmap=None):\n",
        "    \"\"\"\n",
        "    Display a list of images in a single figure with matplotlib.\n",
        "        Parameters:\n",
        "            images: List of np.arrays compatible with plt.imshow.\n",
        "            cols (Default = 2): Number of columns in the figure.\n",
        "            rows (Default = 5): Number of rows in the figure.\n",
        "            cmap (Default = None): Used to display gray images.\n",
        "    \"\"\"\n",
        "    plt.figure(figsize=(10, 11))\n",
        "    for i, image in enumerate(images):\n",
        "        plt.subplot(rows, cols, i+1)\n",
        "        #Use gray scale color map if there is only one channel\n",
        "        cmap = 'gray' if len(image.shape) == 2 else cmap\n",
        "        plt.imshow(image, cmap = cmap)\n",
        "        plt.xticks([])\n",
        "        plt.yticks([])\n",
        "    plt.tight_layout(pad=0, h_pad=0, w_pad=0)\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "XVBrM_vVR40h"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Reading in the test images\n",
        "test_images = [plt.imread(img) for img in glob.glob('test_images/*.jpg')]\n",
        "list_images(test_images)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "JhS05fVnTEeG",
        "outputId": "418dec4e-21eb-4af6-ec23-5fde7e820383"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x1100 with 0 Axes>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Color Selection**"
      ],
      "metadata": {
        "id": "z9bDW98YTTnW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Lane lines in the test images are in white and yellow. We need to choose the most suitable color space, that clearly highlights the lane lines.\n",
        "\n",
        "Original RGB color selection\n",
        "\n",
        "\n",
        "I will apply color selection to the test_images in the original RGB format. We will try to retain as much of the lane lines as possible, while blacking out most of the other stuff."
      ],
      "metadata": {
        "id": "M3iPfZC9Tbfe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def RGB_color_selection(image):\n",
        "    \"\"\"\n",
        "    Apply color selection to RGB images to blackout everything except for white and yellow lane lines.\n",
        "        Parameters:\n",
        "            image: An np.array compatible with plt.imshow.\n",
        "    \"\"\"\n",
        "    #White color mask\n",
        "    lower_threshold = np.uint8([200, 200, 200])\n",
        "    upper_threshold = np.uint8([255, 255, 255])\n",
        "    white_mask = cv2.inRange(image, lower_threshold, upper_threshold)\n",
        "\n",
        "    #Yellow color mask\n",
        "    lower_threshold = np.uint8([175, 175,   0])\n",
        "    upper_threshold = np.uint8([255, 255, 255])\n",
        "    yellow_mask = cv2.inRange(image, lower_threshold, upper_threshold)\n",
        "\n",
        "    #Combine white and yellow masks\n",
        "    mask = cv2.bitwise_or(white_mask, yellow_mask)\n",
        "    masked_image = cv2.bitwise_and(image, image, mask = mask)\n",
        "\n",
        "    return masked_image"
      ],
      "metadata": {
        "id": "EU9LmG1GTJ2J"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Applying color selection to test_images in the RGB color space."
      ],
      "metadata": {
        "id": "zrQNwMHaTqs4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "list_images(list(map(RGB_color_selection, test_images)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "yT9vCohjTmSa",
        "outputId": "b35f86bb-b138-493e-8688-efde93380129"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x1100 with 0 Axes>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**a) HSV color space**\n",
        "\n",
        "Wikipedia: HSV is an alternative representation of the RGB color model. The HSV representation models the way colors mix together, with the saturation dimension resembling various shades of brightly colored paint, and the value dimension resembling the mixture of those paints with varying amounts of black or white."
      ],
      "metadata": {
        "id": "s0dNfDEOT8me"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def convert_hsv(image):\n",
        "    \"\"\"\n",
        "    Convert RGB images to HSV.\n",
        "        Parameters:\n",
        "            image: An np.array compatible with plt.imshow.\n",
        "    \"\"\"\n",
        "    return cv2.cvtColor(image, cv2.COLOR_RGB2HSV)\n",
        "\n",
        "list_images(list(map(convert_hsv, test_images)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "M4CneHJjTwBL",
        "outputId": "a32f47d6-adda-49ee-ac1e-b5b021f223cd"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x1100 with 0 Axes>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def HSV_color_selection(image):\n",
        "    \"\"\"\n",
        "    Apply color selection to the HSV images to blackout everything except for white and yellow lane lines.\n",
        "        Parameters:\n",
        "            image: An np.array compatible with plt.imshow.\n",
        "    \"\"\"\n",
        "    #Convert the input image to HSV\n",
        "    converted_image = convert_hsv(image)\n",
        "\n",
        "    #White color mask\n",
        "    lower_threshold = np.uint8([0, 0, 210])\n",
        "    upper_threshold = np.uint8([255, 30, 255])\n",
        "    white_mask = cv2.inRange(converted_image, lower_threshold, upper_threshold)\n",
        "\n",
        "    #Yellow color mask\n",
        "    lower_threshold = np.uint8([18, 80, 80])\n",
        "    upper_threshold = np.uint8([30, 255, 255])\n",
        "    yellow_mask = cv2.inRange(converted_image, lower_threshold, upper_threshold)\n",
        "\n",
        "    #Combine white and yellow masks\n",
        "    mask = cv2.bitwise_or(white_mask, yellow_mask)\n",
        "    masked_image = cv2.bitwise_and(image, image, mask = mask)\n",
        "\n",
        "    return masked_image"
      ],
      "metadata": {
        "id": "7qg5GXWHUFCG"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Applying color selection to test_images in the HSV color space."
      ],
      "metadata": {
        "id": "6a_nEcvxUPbg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "list_images(list(map(HSV_color_selection, test_images)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "vQxezvmYUKPD",
        "outputId": "dd6a204c-b307-4fad-df10-e3dc6b3a33c0"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x1100 with 0 Axes>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**b) HSL color space**\n",
        "\n",
        "Wikipedia: HSL is an alternative representation of the RGB color model. The HSL model attempts to resemble more perceptual color models such as NCS or Munsell, placing fully saturated colors around a circle at a lightness value of 1/2, where a lightness value of 0 or 1 is fully black or white, respectively."
      ],
      "metadata": {
        "id": "0qOlbAXEUa7J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def convert_hsl(image):\n",
        "    \"\"\"\n",
        "    Convert RGB images to HSL.\n",
        "        Parameters:\n",
        "            image: An np.array compatible with plt.imshow.\n",
        "    \"\"\"\n",
        "    return cv2.cvtColor(image, cv2.COLOR_RGB2HLS)\n",
        "\n",
        "list_images(list(map(convert_hsl, test_images)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "qVOgk7T0USn1",
        "outputId": "53cb0fff-e467-4ace-daec-7f92a673639b"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x1100 with 0 Axes>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def HSL_color_selection(image):\n",
        "    \"\"\"\n",
        "    Apply color selection to the HSL images to blackout everything except for white and yellow lane lines.\n",
        "        Parameters:\n",
        "            image: An np.array compatible with plt.imshow.\n",
        "    \"\"\"\n",
        "    #Convert the input image to HSL\n",
        "    converted_image = convert_hsl(image)\n",
        "\n",
        "    #White color mask\n",
        "    lower_threshold = np.uint8([0, 200, 0])\n",
        "    upper_threshold = np.uint8([255, 255, 255])\n",
        "    white_mask = cv2.inRange(converted_image, lower_threshold, upper_threshold)\n",
        "\n",
        "    #Yellow color mask\n",
        "    lower_threshold = np.uint8([10, 0, 100])\n",
        "    upper_threshold = np.uint8([40, 255, 255])\n",
        "    yellow_mask = cv2.inRange(converted_image, lower_threshold, upper_threshold)\n",
        "\n",
        "    #Combine white and yellow masks\n",
        "    mask = cv2.bitwise_or(white_mask, yellow_mask)\n",
        "    masked_image = cv2.bitwise_and(image, image, mask = mask)\n",
        "\n",
        "    return masked_image"
      ],
      "metadata": {
        "id": "WaTwbP7UUjOE"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Applying color selection to test_images in the HSL color space."
      ],
      "metadata": {
        "id": "zmohUiXBU0qC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "list_images(list(map(HSL_color_selection, test_images)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "r1H_LJFGUt2a",
        "outputId": "b4496ff2-2236-4a0f-febd-325eeb92d071"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x1100 with 0 Axes>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using HSL produces the clearest lane lines of all color spaces. We will use them for the next steps."
      ],
      "metadata": {
        "id": "h1n-UgmKU-Zy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "color_selected_images = list(map(HSL_color_selection, test_images))"
      ],
      "metadata": {
        "id": "qUhddABbU39h"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**3. Canny Edge Detection**\n",
        "\n",
        "Wikipedia: The Canny edge detector is an edge detection operator that uses a multi-stage algorithm to detect a wide range of edges in images.\n",
        "\n",
        "**a) Gray scaling the images**\n",
        "\n",
        "The Canny edge detection algorithm measures the intensity gradients of each pixel. So, we need to convert the images into gray scale in order to detect edges."
      ],
      "metadata": {
        "id": "aIs0Bw4YVKDc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def gray_scale(image):\n",
        "    \"\"\"\n",
        "    Convert images to gray scale.\n",
        "        Parameters:\n",
        "            image: An np.array compatible with plt.imshow.\n",
        "    \"\"\"\n",
        "    return cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)"
      ],
      "metadata": {
        "id": "e6BurzxJVCR4"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gray_images = list(map(gray_scale, color_selected_images))\n",
        "list_images(gray_images)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "bmkEtlYbVVC2",
        "outputId": "67c148c7-351b-44a8-9fd1-54d52df63794"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x1100 with 0 Axes>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**b) Applying Gaussian smoothing**\n",
        "\n",
        "Wikipedia: Since all edge detection results are easily affected by image noise, it is essential to filter out the noise to prevent false detection caused by noise. To smooth the image, a Gaussian filter is applied to convolve with the image. This step will slightly smooth the image to reduce the effects of obvious noise on the edge detector."
      ],
      "metadata": {
        "id": "M9nJltgqVkXn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def gaussian_smoothing(image, kernel_size = 13):\n",
        "    \"\"\"\n",
        "    Apply Gaussian filter to the input image.\n",
        "        Parameters:\n",
        "            image: An np.array compatible with plt.imshow.\n",
        "            kernel_size (Default = 13): The size of the Gaussian kernel will affect the performance of the detector.\n",
        "            It must be an odd number (3, 5, 7, ...).\n",
        "    \"\"\"\n",
        "    return cv2.GaussianBlur(image, (kernel_size, kernel_size), 0)"
      ],
      "metadata": {
        "id": "fNzLU7xUVYpe"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "blur_images = list(map(gaussian_smoothing, gray_images))\n",
        "list_images(blur_images)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "ewsmOL8mVrUH",
        "outputId": "aba3c66e-a075-4e77-d3bc-7f36e33e6a0f"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x1100 with 0 Axes>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**c) Applying Canny Edge Detection**\n",
        "\n",
        "Wikipedia: The Process of Canny edge detection algorithm can be broken down to 5 different steps:\n",
        "\n",
        "Find the intensity gradients of the image\n",
        "Apply non-maximum suppression to get rid of spurious response to edge detection.\n",
        "Apply double threshold to determine potential edges.\n",
        "Track edge by hysteresis: Finalize the detection of edges by suppressing all the other edges that are weak and not connected to strong edges.\n",
        "\n",
        "*If an edge pixel’s gradient value is higher than the high threshold value, it is marked as a strong edge pixel. If an edge pixel’s gradient value is smaller than the high threshold value and larger than the low threshold value, it is marked as a weak edge pixel. If an edge pixel's value is smaller than the low threshold value, it will be suppressed. The two threshold values are empirically determined and their definition will depend on the content of a given input image."
      ],
      "metadata": {
        "id": "rsoBY-gtV7gW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def canny_detector(image, low_threshold = 50, high_threshold = 150):\n",
        "    \"\"\"\n",
        "    Apply Canny Edge Detection algorithm to the input image.\n",
        "        Parameters:\n",
        "            image: An np.array compatible with plt.imshow.\n",
        "            low_threshold (Default = 50).\n",
        "            high_threshold (Default = 150).\n",
        "    \"\"\""
      ],
      "metadata": {
        "id": "q7wZW69pVu0b"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "edge_detected_images = list(map(canny_detector, blur_images))\n",
        "list_images(edge_detected_images)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "CSDUArbxWFal",
        "outputId": "734660fb-7b61-4397-eeb3-aa42d009a3c3"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x1100 with 0 Axes>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**4. Region of interest**\n",
        "\n",
        "We're interested in the area facing the camera, where the lane lines are found. So, we'll apply region masking to cut out everything else."
      ],
      "metadata": {
        "id": "y4i3Qt6TWNih"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def region_selection(image):\n",
        "    \"\"\"\n",
        "    Determine and cut the region of interest in the input image.\n",
        "        Parameters:\n",
        "            image: An np.array compatible with plt.imshow.\n",
        "    \"\"\"\n",
        "    mask = np.zeros_like(image)\n",
        "    #Defining a 3 channel or 1 channel color to fill the mask with depending on the input image\n",
        "    if len(image.shape) > 2:\n",
        "        channel_count = image.shape[2]\n",
        "        ignore_mask_color = (255,) * channel_count\n",
        "    else:\n",
        "        ignore_mask_color = 255\n",
        "    #We could have used fixed numbers as the vertices of the polygon,\n",
        "    #but they will not be applicable to images with different dimesnions.\n",
        "    rows, cols = image.shape[:2]\n",
        "    bottom_left  = [cols * 0.1, rows * 0.95]\n",
        "    top_left     = [cols * 0.4, rows * 0.6]\n",
        "    bottom_right = [cols * 0.9, rows * 0.95]\n",
        "    top_right    = [cols * 0.6, rows * 0.6]\n",
        "    vertices = np.array([[bottom_left, top_left, top_right, bottom_right]], dtype=np.int32)\n",
        "    cv2.fillPoly(mask, vertices, ignore_mask_color)\n",
        "    masked_image = cv2.bitwise_and(image, mask)\n",
        "    return masked_image"
      ],
      "metadata": {
        "id": "ejnn2JgbWISq"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "masked_image = list(map(region_selection, edge_detected_images))\n",
        "list_images(masked_image)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "YlreH_gjWXCD",
        "outputId": "b2c86e14-08ff-485e-e99b-c8449a01817f"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x1100 with 0 Axes>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**5. Hough Transform**\n",
        "\n",
        "The Hough transform is a technique which can be used to isolate features of a particular shape within an image. I'll use it to detected the lane lines in selected_region_images."
      ],
      "metadata": {
        "id": "OQ8SyCFVWfBw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def hough_transform(image):\n",
        "    \"\"\"\n",
        "    Determine and cut the region of interest in the input image.\n",
        "        Parameters:\n",
        "            image: The output of a Canny transform.\n",
        "    \"\"\"\n",
        "    rho = 1              #Distance resolution of the accumulator in pixels.\n",
        "    theta = np.pi/180    #Angle resolution of the accumulator in radians.\n",
        "    threshold = 20       #Only lines that are greater than threshold will be returned.\n",
        "    minLineLength = 20   #Line segments shorter than that are rejected.\n",
        "    maxLineGap = 300     #Maximum allowed gap between points on the same line to link them\n",
        "    return cv2.HoughLinesP(image, rho = rho, theta = theta, threshold = threshold,\n",
        "                           minLineLength = minLineLength, maxLineGap = maxLineGap)"
      ],
      "metadata": {
        "id": "Ggdu9pvrWaHx"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "hough_lines contains the list of lines detected in the selected region. Now, we will draw these detected lines onto the original test_images."
      ],
      "metadata": {
        "id": "q4ryf-iOWvo2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "hough_lines = list(map(hough_transform, masked_image))"
      ],
      "metadata": {
        "id": "YoyIrja5WleZ"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def draw_lines(image, lines, color = [255, 0, 0], thickness = 2):\n",
        "    \"\"\"\n",
        "    Draw lines onto the input image.\n",
        "        Parameters:\n",
        "            image: An np.array compatible with plt.imshow.\n",
        "            lines: The lines we want to draw.\n",
        "            color (Default = red): Line color.\n",
        "            thickness (Default = 2): Line thickness.\n",
        "    \"\"\"\n",
        "    image = np.copy(image)\n",
        "    for line in lines:\n",
        "        for x1,y1,x2,y2 in line:\n",
        "            cv2.line(image, (x1, y1), (x2, y2), color, thickness)\n",
        "    return image"
      ],
      "metadata": {
        "id": "zgAibyHDWzsS"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "line_images = []\n",
        "for image, lines in zip(test_images, hough_lines):\n",
        "    line_images.append(draw_lines(image, lines))\n",
        "\n",
        "list_images(line_images)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "8byrkI7tW3yj",
        "outputId": "a7c74329-db8e-4189-90ce-c5d1820f786e"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x1100 with 0 Axes>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**6. Averaging and extrapolating the lane lines**\n",
        "\n",
        "We have multiple lines detected for each lane line. We need to average all these lines and draw a single line for each lane line. We also need to extrapolate the lane lines to cover the full lane line length."
      ],
      "metadata": {
        "id": "uYKpkMWrW_s1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def average_slope_intercept(lines):\n",
        "    \"\"\"\n",
        "    Find the slope and intercept of the left and right lanes of each image.\n",
        "        Parameters:\n",
        "            lines: The output lines from Hough Transform.\n",
        "    \"\"\"\n",
        "    left_lines    = [] #(slope, intercept)\n",
        "    left_weights  = [] #(length,)\n",
        "    right_lines   = [] #(slope, intercept)\n",
        "    right_weights = [] #(length,)\n",
        "\n",
        "    for line in lines:\n",
        "        for x1, y1, x2, y2 in line:\n",
        "            if x1 == x2:\n",
        "                continue\n",
        "            slope = (y2 - y1) / (x2 - x1)\n",
        "            intercept = y1 - (slope * x1)\n",
        "            length = np.sqrt(((y2 - y1) ** 2) + ((x2 - x1) ** 2))\n",
        "            if slope < 0:\n",
        "                left_lines.append((slope, intercept))\n",
        "                left_weights.append((length))\n",
        "            else:\n",
        "                right_lines.append((slope, intercept))\n",
        "                right_weights.append((length))\n",
        "    left_lane  = np.dot(left_weights,  left_lines) / np.sum(left_weights)  if len(left_weights) > 0 else None\n",
        "    right_lane = np.dot(right_weights, right_lines) / np.sum(right_weights) if len(right_weights) > 0 else None\n",
        "    return left_lane, right_lane"
      ],
      "metadata": {
        "id": "igp78jWuW6zT"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def pixel_points(y1, y2, line):\n",
        "    \"\"\"\n",
        "    Converts the slope and intercept of each line into pixel points.\n",
        "        Parameters:\n",
        "            y1: y-value of the line's starting point.\n",
        "            y2: y-value of the line's end point.\n",
        "            line: The slope and intercept of the line.\n",
        "    \"\"\"\n",
        "    if line is None:\n",
        "        return None\n",
        "    slope, intercept = line\n",
        "    x1 = int((y1 - intercept)/slope)\n",
        "    x2 = int((y2 - intercept)/slope)\n",
        "    y1 = int(y1)\n",
        "    y2 = int(y2)\n",
        "    return ((x1, y1), (x2, y2))"
      ],
      "metadata": {
        "id": "pAnK49XtXI43"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def lane_lines(image, lines):\n",
        "    \"\"\"\n",
        "    Create full lenght lines from pixel points.\n",
        "        Parameters:\n",
        "            image: The input test image.\n",
        "            lines: The output lines from Hough Transform.\n",
        "    \"\"\"\n",
        "    left_lane, right_lane = average_slope_intercept(lines)\n",
        "    y1 = image.shape[0]\n",
        "    y2 = y1 * 0.6\n",
        "    left_line  = pixel_points(y1, y2, left_lane)\n",
        "    right_line = pixel_points(y1, y2, right_lane)\n",
        "    return left_line, right_line\n",
        "\n",
        "\n",
        "def draw_lane_lines(image, lines, color=[255, 0, 0], thickness=12):\n",
        "    \"\"\"\n",
        "    Draw lines onto the input image.\n",
        "        Parameters:\n",
        "            image: The input test image.\n",
        "            lines: The output lines from Hough Transform.\n",
        "            color (Default = red): Line color.\n",
        "            thickness (Default = 12): Line thickness.\n",
        "    \"\"\"\n",
        "    line_image = np.zeros_like(image)\n",
        "    for line in lines:\n",
        "        if line is not None:\n",
        "            cv2.line(line_image, *line,  color, thickness)\n",
        "    return cv2.addWeighted(image, 1.0, line_image, 1.0, 0.0)\n",
        "\n",
        "\n",
        "lane_images = []\n",
        "for image, lines in zip(test_images, hough_lines):\n",
        "    lane_images.append(draw_lane_lines(image, lane_lines(image, lines)))\n",
        "\n",
        "\n",
        "list_images(lane_images)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "YGnAYfyQXNVL",
        "outputId": "fbb910e6-5df6-40f2-a80a-b329bcf478db"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x1100 with 0 Axes>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**7. Apply on video streams**\n",
        "\n",
        "Now, we'll use the above functions to detect lane lines from a video stream."
      ],
      "metadata": {
        "id": "Q73HAhJlXYa6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Import everything needed to edit/save/watch video clips\n",
        "from moviepy import *\n",
        "from IPython.display import HTML\n",
        "from IPython.display import Image"
      ],
      "metadata": {
        "id": "8QoH_InyXTLY"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def frame_processor(image):\n",
        "    \"\"\"\n",
        "    Process the input frame to detect lane lines.\n",
        "        Parameters:\n",
        "            image: Single video frame.\n",
        "    \"\"\"\n",
        "    color_select = HSL_color_selection(image)\n",
        "    gray         = gray_scale(color_select)\n",
        "    smooth       = gaussian_smoothing(gray)\n",
        "    edges        = canny_detector(smooth)\n",
        "    region       = region_selection(edges)\n",
        "    hough        = hough_transform(region)\n",
        "    result       = draw_lane_lines(image, lane_lines(image, hough))\n",
        "    return result"
      ],
      "metadata": {
        "id": "94QJpPscXd6y"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def process_video(test_video, output_video):\n",
        "    \"\"\"\n",
        "    Read input video stream and produce a video file with detected lane lines.\n",
        "        Parameters:\n",
        "            test_video: Input video.\n",
        "            output_video: A video file with detected lane lines.\n",
        "    \"\"\"\n",
        "    input_video = VideoFileClip(os.path.join('test_videos', test_video), audio=False)\n",
        "    processed = input_video.fl_image(frame_processor)\n",
        "    processed.write_videofile(os.path.join('output_videos', output_video), audio=False)"
      ],
      "metadata": {
        "id": "8PSHC-r_Xip6"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%time process_video('solidWhiteRight.mp4', 'solidWhiteRight_output.mp4')\n",
        "HTML(\"\"\"\n",
        "<video width=\"960\" height=\"540\" controls>\n",
        "  <source src=\"{0}\">\n",
        "</video>\n",
        "\"\"\".format(\"output_videos\\solidWhiteRight_output.mp4\"))"
      ],
      "metadata": {
        "colab": {
          "resources": {
            "http://localhost:8080/output_videos/solidWhiteRight_output.mp4": {
              "data": "CjwhRE9DVFlQRSBodG1sPgo8aHRtbCBsYW5nPWVuPgogIDxtZXRhIGNoYXJzZXQ9dXRmLTg+CiAgPG1ldGEgbmFtZT12aWV3cG9ydCBjb250ZW50PSJpbml0aWFsLXNjYWxlPTEsIG1pbmltdW0tc2NhbGU9MSwgd2lkdGg9ZGV2aWNlLXdpZHRoIj4KICA8dGl0bGU+RXJyb3IgNDA0IChOb3QgRm91bmQpISExPC90aXRsZT4KICA8c3R5bGU+CiAgICAqe21hcmdpbjowO3BhZGRpbmc6MH1odG1sLGNvZGV7Zm9udDoxNXB4LzIycHggYXJpYWwsc2Fucy1zZXJpZn1odG1se2JhY2tncm91bmQ6I2ZmZjtjb2xvcjojMjIyO3BhZGRpbmc6MTVweH1ib2R5e21hcmdpbjo3JSBhdXRvIDA7bWF4LXdpZHRoOjM5MHB4O21pbi1oZWlnaHQ6MTgwcHg7cGFkZGluZzozMHB4IDAgMTVweH0qID4gYm9keXtiYWNrZ3JvdW5kOnVybCgvL3d3dy5nb29nbGUuY29tL2ltYWdlcy9lcnJvcnMvcm9ib3QucG5nKSAxMDAlIDVweCBuby1yZXBlYXQ7cGFkZGluZy1yaWdodDoyMDVweH1we21hcmdpbjoxMXB4IDAgMjJweDtvdmVyZmxvdzpoaWRkZW59aW5ze2NvbG9yOiM3Nzc7dGV4dC1kZWNvcmF0aW9uOm5vbmV9YSBpbWd7Ym9yZGVyOjB9QG1lZGlhIHNjcmVlbiBhbmQgKG1heC13aWR0aDo3NzJweCl7Ym9keXtiYWNrZ3JvdW5kOm5vbmU7bWFyZ2luLXRvcDowO21heC13aWR0aDpub25lO3BhZGRpbmctcmlnaHQ6MH19I2xvZ297YmFja2dyb3VuZDp1cmwoLy93d3cuZ29vZ2xlLmNvbS9pbWFnZXMvbG9nb3MvZXJyb3JwYWdlL2Vycm9yX2xvZ28tMTUweDU0LnBuZykgbm8tcmVwZWF0O21hcmdpbi1sZWZ0Oi01cHh9QG1lZGlhIG9ubHkgc2NyZWVuIGFuZCAobWluLXJlc29sdXRpb246MTkyZHBpKXsjbG9nb3tiYWNrZ3JvdW5kOnVybCgvL3d3dy5nb29nbGUuY29tL2ltYWdlcy9sb2dvcy9lcnJvcnBhZ2UvZXJyb3JfbG9nby0xNTB4NTQtMngucG5nKSBuby1yZXBlYXQgMCUgMCUvMTAwJSAxMDAlOy1tb3otYm9yZGVyLWltYWdlOnVybCgvL3d3dy5nb29nbGUuY29tL2ltYWdlcy9sb2dvcy9lcnJvcnBhZ2UvZXJyb3JfbG9nby0xNTB4NTQtMngucG5nKSAwfX1AbWVkaWEgb25seSBzY3JlZW4gYW5kICgtd2Via2l0LW1pbi1kZXZpY2UtcGl4ZWwtcmF0aW86Mil7I2xvZ297YmFja2dyb3VuZDp1cmwoLy93d3cuZ29vZ2xlLmNvbS9pbWFnZXMvbG9nb3MvZXJyb3JwYWdlL2Vycm9yX2xvZ28tMTUweDU0LTJ4LnBuZykgbm8tcmVwZWF0Oy13ZWJraXQtYmFja2dyb3VuZC1zaXplOjEwMCUgMTAwJX19I2xvZ297ZGlzcGxheTppbmxpbmUtYmxvY2s7aGVpZ2h0OjU0cHg7d2lkdGg6MTUwcHh9CiAgPC9zdHlsZT4KICA8YSBocmVmPS8vd3d3Lmdvb2dsZS5jb20vPjxzcGFuIGlkPWxvZ28gYXJpYS1sYWJlbD1Hb29nbGU+PC9zcGFuPjwvYT4KICA8cD48Yj40MDQuPC9iPiA8aW5zPlRoYXTigJlzIGFuIGVycm9yLjwvaW5zPgogIDxwPiAgPGlucz5UaGF04oCZcyBhbGwgd2Uga25vdy48L2lucz4K",
              "ok": false,
              "headers": [
                [
                  "content-length",
                  "1449"
                ],
                [
                  "content-type",
                  "text/html; charset=utf-8"
                ]
              ],
              "status": 404,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "KlcEQOnWXqnW",
        "outputId": "c2bb4b3e-80c8-4c47-c306-5ade246e1ebf"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "error",
          "ename": "OSError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
            "\u001b[0;32m<timed eval>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-30-bcf0975daa00>\u001b[0m in \u001b[0;36mprocess_video\u001b[0;34m(test_video, output_video)\u001b[0m\n\u001b[1;32m      6\u001b[0m             \u001b[0moutput_video\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mA\u001b[0m \u001b[0mvideo\u001b[0m \u001b[0mfile\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mdetected\u001b[0m \u001b[0mlane\u001b[0m \u001b[0mlines\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \"\"\"\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0minput_video\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVideoFileClip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'test_videos'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_video\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maudio\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m     \u001b[0mprocessed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput_video\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfl_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe_processor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mprocessed\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite_videofile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'output_videos'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_video\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maudio\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/moviepy/video/io/VideoFileClip.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, filename, has_mask, audio, audio_buffersize, target_resolution, resize_algorithm, audio_fps, audio_nbytes, verbose, fps_source)\u001b[0m\n\u001b[1;32m     86\u001b[0m         \u001b[0;31m# Make a reader\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m         \u001b[0mpix_fmt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"rgba\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mhas_mask\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"rgb24\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m         self.reader = FFMPEG_VideoReader(filename, pix_fmt=pix_fmt,\n\u001b[0m\u001b[1;32m     89\u001b[0m                                          \u001b[0mtarget_resolution\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtarget_resolution\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m                                          \u001b[0mresize_algo\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mresize_algorithm\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/moviepy/video/io/ffmpeg_reader.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, filename, print_infos, bufsize, pix_fmt, check_duration, target_resolution, resize_algo, fps_source)\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilename\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mproc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m         infos = ffmpeg_parse_infos(filename, print_infos, check_duration,\n\u001b[0m\u001b[1;32m     36\u001b[0m                                    fps_source)\n\u001b[1;32m     37\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minfos\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'video_fps'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/moviepy/video/io/ffmpeg_reader.py\u001b[0m in \u001b[0;36mffmpeg_parse_infos\u001b[0;34m(filename, print_infos, check_duration, fps_source)\u001b[0m\n\u001b[1;32m    268\u001b[0m     \u001b[0mlines\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minfos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplitlines\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    269\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;34m\"No such file or directory\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlines\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 270\u001b[0;31m         raise IOError((\"MoviePy error: the file %s could not be found!\\n\"\n\u001b[0m\u001b[1;32m    271\u001b[0m                       \u001b[0;34m\"Please check that you entered the correct \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    272\u001b[0m                       \"path.\")%filename)\n",
            "\u001b[0;31mOSError\u001b[0m: MoviePy error: the file test_videos/solidWhiteRight.mp4 could not be found!\nPlease check that you entered the correct path."
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "<video width=\"960\" height=\"540\" controls>\n",
              "  <source src=\"output_videos\\solidWhiteRight_output.mp4\">\n",
              "</video>\n"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%time process_video('solidYellowLeft.mp4', 'solidYellowLeft_output.mp4')\n",
        "HTML(\"\"\"\n",
        "<video width=\"960\" height=\"540\" controls>\n",
        "  <source src=\"{0}\">\n",
        "</video>\n",
        "\"\"\".format(\"output_videos\\solidYellowLeft_output.mp4\"))"
      ],
      "metadata": {
        "colab": {
          "resources": {
            "http://localhost:8080/output_videos/solidYellowLeft_output.mp4": {
              "data": "CjwhRE9DVFlQRSBodG1sPgo8aHRtbCBsYW5nPWVuPgogIDxtZXRhIGNoYXJzZXQ9dXRmLTg+CiAgPG1ldGEgbmFtZT12aWV3cG9ydCBjb250ZW50PSJpbml0aWFsLXNjYWxlPTEsIG1pbmltdW0tc2NhbGU9MSwgd2lkdGg9ZGV2aWNlLXdpZHRoIj4KICA8dGl0bGU+RXJyb3IgNDA0IChOb3QgRm91bmQpISExPC90aXRsZT4KICA8c3R5bGU+CiAgICAqe21hcmdpbjowO3BhZGRpbmc6MH1odG1sLGNvZGV7Zm9udDoxNXB4LzIycHggYXJpYWwsc2Fucy1zZXJpZn1odG1se2JhY2tncm91bmQ6I2ZmZjtjb2xvcjojMjIyO3BhZGRpbmc6MTVweH1ib2R5e21hcmdpbjo3JSBhdXRvIDA7bWF4LXdpZHRoOjM5MHB4O21pbi1oZWlnaHQ6MTgwcHg7cGFkZGluZzozMHB4IDAgMTVweH0qID4gYm9keXtiYWNrZ3JvdW5kOnVybCgvL3d3dy5nb29nbGUuY29tL2ltYWdlcy9lcnJvcnMvcm9ib3QucG5nKSAxMDAlIDVweCBuby1yZXBlYXQ7cGFkZGluZy1yaWdodDoyMDVweH1we21hcmdpbjoxMXB4IDAgMjJweDtvdmVyZmxvdzpoaWRkZW59aW5ze2NvbG9yOiM3Nzc7dGV4dC1kZWNvcmF0aW9uOm5vbmV9YSBpbWd7Ym9yZGVyOjB9QG1lZGlhIHNjcmVlbiBhbmQgKG1heC13aWR0aDo3NzJweCl7Ym9keXtiYWNrZ3JvdW5kOm5vbmU7bWFyZ2luLXRvcDowO21heC13aWR0aDpub25lO3BhZGRpbmctcmlnaHQ6MH19I2xvZ297YmFja2dyb3VuZDp1cmwoLy93d3cuZ29vZ2xlLmNvbS9pbWFnZXMvbG9nb3MvZXJyb3JwYWdlL2Vycm9yX2xvZ28tMTUweDU0LnBuZykgbm8tcmVwZWF0O21hcmdpbi1sZWZ0Oi01cHh9QG1lZGlhIG9ubHkgc2NyZWVuIGFuZCAobWluLXJlc29sdXRpb246MTkyZHBpKXsjbG9nb3tiYWNrZ3JvdW5kOnVybCgvL3d3dy5nb29nbGUuY29tL2ltYWdlcy9sb2dvcy9lcnJvcnBhZ2UvZXJyb3JfbG9nby0xNTB4NTQtMngucG5nKSBuby1yZXBlYXQgMCUgMCUvMTAwJSAxMDAlOy1tb3otYm9yZGVyLWltYWdlOnVybCgvL3d3dy5nb29nbGUuY29tL2ltYWdlcy9sb2dvcy9lcnJvcnBhZ2UvZXJyb3JfbG9nby0xNTB4NTQtMngucG5nKSAwfX1AbWVkaWEgb25seSBzY3JlZW4gYW5kICgtd2Via2l0LW1pbi1kZXZpY2UtcGl4ZWwtcmF0aW86Mil7I2xvZ297YmFja2dyb3VuZDp1cmwoLy93d3cuZ29vZ2xlLmNvbS9pbWFnZXMvbG9nb3MvZXJyb3JwYWdlL2Vycm9yX2xvZ28tMTUweDU0LTJ4LnBuZykgbm8tcmVwZWF0Oy13ZWJraXQtYmFja2dyb3VuZC1zaXplOjEwMCUgMTAwJX19I2xvZ297ZGlzcGxheTppbmxpbmUtYmxvY2s7aGVpZ2h0OjU0cHg7d2lkdGg6MTUwcHh9CiAgPC9zdHlsZT4KICA8YSBocmVmPS8vd3d3Lmdvb2dsZS5jb20vPjxzcGFuIGlkPWxvZ28gYXJpYS1sYWJlbD1Hb29nbGU+PC9zcGFuPjwvYT4KICA8cD48Yj40MDQuPC9iPiA8aW5zPlRoYXTigJlzIGFuIGVycm9yLjwvaW5zPgogIDxwPiAgPGlucz5UaGF04oCZcyBhbGwgd2Uga25vdy48L2lucz4K",
              "ok": false,
              "headers": [
                [
                  "content-length",
                  "1449"
                ],
                [
                  "content-type",
                  "text/html; charset=utf-8"
                ]
              ],
              "status": 404,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "AfccCqGjXvDp",
        "outputId": "b089ba81-845f-47a0-ff4d-839659070ab5"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "error",
          "ename": "OSError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
            "\u001b[0;32m<timed eval>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-30-bcf0975daa00>\u001b[0m in \u001b[0;36mprocess_video\u001b[0;34m(test_video, output_video)\u001b[0m\n\u001b[1;32m      6\u001b[0m             \u001b[0moutput_video\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mA\u001b[0m \u001b[0mvideo\u001b[0m \u001b[0mfile\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mdetected\u001b[0m \u001b[0mlane\u001b[0m \u001b[0mlines\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \"\"\"\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0minput_video\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVideoFileClip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'test_videos'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_video\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maudio\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m     \u001b[0mprocessed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput_video\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfl_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe_processor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mprocessed\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite_videofile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'output_videos'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_video\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maudio\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/moviepy/video/io/VideoFileClip.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, filename, has_mask, audio, audio_buffersize, target_resolution, resize_algorithm, audio_fps, audio_nbytes, verbose, fps_source)\u001b[0m\n\u001b[1;32m     86\u001b[0m         \u001b[0;31m# Make a reader\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m         \u001b[0mpix_fmt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"rgba\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mhas_mask\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"rgb24\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m         self.reader = FFMPEG_VideoReader(filename, pix_fmt=pix_fmt,\n\u001b[0m\u001b[1;32m     89\u001b[0m                                          \u001b[0mtarget_resolution\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtarget_resolution\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m                                          \u001b[0mresize_algo\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mresize_algorithm\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/moviepy/video/io/ffmpeg_reader.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, filename, print_infos, bufsize, pix_fmt, check_duration, target_resolution, resize_algo, fps_source)\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilename\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mproc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m         infos = ffmpeg_parse_infos(filename, print_infos, check_duration,\n\u001b[0m\u001b[1;32m     36\u001b[0m                                    fps_source)\n\u001b[1;32m     37\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minfos\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'video_fps'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/moviepy/video/io/ffmpeg_reader.py\u001b[0m in \u001b[0;36mffmpeg_parse_infos\u001b[0;34m(filename, print_infos, check_duration, fps_source)\u001b[0m\n\u001b[1;32m    268\u001b[0m     \u001b[0mlines\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minfos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplitlines\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    269\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;34m\"No such file or directory\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlines\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 270\u001b[0;31m         raise IOError((\"MoviePy error: the file %s could not be found!\\n\"\n\u001b[0m\u001b[1;32m    271\u001b[0m                       \u001b[0;34m\"Please check that you entered the correct \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    272\u001b[0m                       \"path.\")%filename)\n",
            "\u001b[0;31mOSError\u001b[0m: MoviePy error: the file test_videos/solidYellowLeft.mp4 could not be found!\nPlease check that you entered the correct path."
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "<video width=\"960\" height=\"540\" controls>\n",
              "  <source src=\"output_videos\\solidYellowLeft_output.mp4\">\n",
              "</video>\n"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%time process_video('challenge.mp4', 'challenge_output.mp4')\n",
        "HTML(\"\"\"\n",
        "<video width=\"960\" height=\"540\" controls>\n",
        "  <source src=\"{0}\">\n",
        "</video>\n",
        "\"\"\".format(\"output_videos\\challenge_output.mp4\"))"
      ],
      "metadata": {
        "colab": {
          "resources": {
            "http://localhost:8080/output_videos/challenge_output.mp4": {
              "data": "CjwhRE9DVFlQRSBodG1sPgo8aHRtbCBsYW5nPWVuPgogIDxtZXRhIGNoYXJzZXQ9dXRmLTg+CiAgPG1ldGEgbmFtZT12aWV3cG9ydCBjb250ZW50PSJpbml0aWFsLXNjYWxlPTEsIG1pbmltdW0tc2NhbGU9MSwgd2lkdGg9ZGV2aWNlLXdpZHRoIj4KICA8dGl0bGU+RXJyb3IgNDA0IChOb3QgRm91bmQpISExPC90aXRsZT4KICA8c3R5bGU+CiAgICAqe21hcmdpbjowO3BhZGRpbmc6MH1odG1sLGNvZGV7Zm9udDoxNXB4LzIycHggYXJpYWwsc2Fucy1zZXJpZn1odG1se2JhY2tncm91bmQ6I2ZmZjtjb2xvcjojMjIyO3BhZGRpbmc6MTVweH1ib2R5e21hcmdpbjo3JSBhdXRvIDA7bWF4LXdpZHRoOjM5MHB4O21pbi1oZWlnaHQ6MTgwcHg7cGFkZGluZzozMHB4IDAgMTVweH0qID4gYm9keXtiYWNrZ3JvdW5kOnVybCgvL3d3dy5nb29nbGUuY29tL2ltYWdlcy9lcnJvcnMvcm9ib3QucG5nKSAxMDAlIDVweCBuby1yZXBlYXQ7cGFkZGluZy1yaWdodDoyMDVweH1we21hcmdpbjoxMXB4IDAgMjJweDtvdmVyZmxvdzpoaWRkZW59aW5ze2NvbG9yOiM3Nzc7dGV4dC1kZWNvcmF0aW9uOm5vbmV9YSBpbWd7Ym9yZGVyOjB9QG1lZGlhIHNjcmVlbiBhbmQgKG1heC13aWR0aDo3NzJweCl7Ym9keXtiYWNrZ3JvdW5kOm5vbmU7bWFyZ2luLXRvcDowO21heC13aWR0aDpub25lO3BhZGRpbmctcmlnaHQ6MH19I2xvZ297YmFja2dyb3VuZDp1cmwoLy93d3cuZ29vZ2xlLmNvbS9pbWFnZXMvbG9nb3MvZXJyb3JwYWdlL2Vycm9yX2xvZ28tMTUweDU0LnBuZykgbm8tcmVwZWF0O21hcmdpbi1sZWZ0Oi01cHh9QG1lZGlhIG9ubHkgc2NyZWVuIGFuZCAobWluLXJlc29sdXRpb246MTkyZHBpKXsjbG9nb3tiYWNrZ3JvdW5kOnVybCgvL3d3dy5nb29nbGUuY29tL2ltYWdlcy9sb2dvcy9lcnJvcnBhZ2UvZXJyb3JfbG9nby0xNTB4NTQtMngucG5nKSBuby1yZXBlYXQgMCUgMCUvMTAwJSAxMDAlOy1tb3otYm9yZGVyLWltYWdlOnVybCgvL3d3dy5nb29nbGUuY29tL2ltYWdlcy9sb2dvcy9lcnJvcnBhZ2UvZXJyb3JfbG9nby0xNTB4NTQtMngucG5nKSAwfX1AbWVkaWEgb25seSBzY3JlZW4gYW5kICgtd2Via2l0LW1pbi1kZXZpY2UtcGl4ZWwtcmF0aW86Mil7I2xvZ297YmFja2dyb3VuZDp1cmwoLy93d3cuZ29vZ2xlLmNvbS9pbWFnZXMvbG9nb3MvZXJyb3JwYWdlL2Vycm9yX2xvZ28tMTUweDU0LTJ4LnBuZykgbm8tcmVwZWF0Oy13ZWJraXQtYmFja2dyb3VuZC1zaXplOjEwMCUgMTAwJX19I2xvZ297ZGlzcGxheTppbmxpbmUtYmxvY2s7aGVpZ2h0OjU0cHg7d2lkdGg6MTUwcHh9CiAgPC9zdHlsZT4KICA8YSBocmVmPS8vd3d3Lmdvb2dsZS5jb20vPjxzcGFuIGlkPWxvZ28gYXJpYS1sYWJlbD1Hb29nbGU+PC9zcGFuPjwvYT4KICA8cD48Yj40MDQuPC9iPiA8aW5zPlRoYXTigJlzIGFuIGVycm9yLjwvaW5zPgogIDxwPiAgPGlucz5UaGF04oCZcyBhbGwgd2Uga25vdy48L2lucz4K",
              "ok": false,
              "headers": [
                [
                  "content-length",
                  "1449"
                ],
                [
                  "content-type",
                  "text/html; charset=utf-8"
                ]
              ],
              "status": 404,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "N6S5GBkRYAZe",
        "outputId": "b6f69b2f-c38b-431a-cd2b-f75cf7859cfe"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "error",
          "ename": "OSError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
            "\u001b[0;32m<timed eval>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-30-bcf0975daa00>\u001b[0m in \u001b[0;36mprocess_video\u001b[0;34m(test_video, output_video)\u001b[0m\n\u001b[1;32m      6\u001b[0m             \u001b[0moutput_video\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mA\u001b[0m \u001b[0mvideo\u001b[0m \u001b[0mfile\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mdetected\u001b[0m \u001b[0mlane\u001b[0m \u001b[0mlines\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \"\"\"\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0minput_video\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVideoFileClip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'test_videos'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_video\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maudio\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m     \u001b[0mprocessed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput_video\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfl_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe_processor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mprocessed\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite_videofile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'output_videos'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_video\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maudio\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/moviepy/video/io/VideoFileClip.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, filename, has_mask, audio, audio_buffersize, target_resolution, resize_algorithm, audio_fps, audio_nbytes, verbose, fps_source)\u001b[0m\n\u001b[1;32m     86\u001b[0m         \u001b[0;31m# Make a reader\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m         \u001b[0mpix_fmt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"rgba\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mhas_mask\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"rgb24\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m         self.reader = FFMPEG_VideoReader(filename, pix_fmt=pix_fmt,\n\u001b[0m\u001b[1;32m     89\u001b[0m                                          \u001b[0mtarget_resolution\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtarget_resolution\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m                                          \u001b[0mresize_algo\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mresize_algorithm\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/moviepy/video/io/ffmpeg_reader.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, filename, print_infos, bufsize, pix_fmt, check_duration, target_resolution, resize_algo, fps_source)\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilename\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mproc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m         infos = ffmpeg_parse_infos(filename, print_infos, check_duration,\n\u001b[0m\u001b[1;32m     36\u001b[0m                                    fps_source)\n\u001b[1;32m     37\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minfos\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'video_fps'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/moviepy/video/io/ffmpeg_reader.py\u001b[0m in \u001b[0;36mffmpeg_parse_infos\u001b[0;34m(filename, print_infos, check_duration, fps_source)\u001b[0m\n\u001b[1;32m    268\u001b[0m     \u001b[0mlines\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minfos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplitlines\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    269\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;34m\"No such file or directory\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlines\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 270\u001b[0;31m         raise IOError((\"MoviePy error: the file %s could not be found!\\n\"\n\u001b[0m\u001b[1;32m    271\u001b[0m                       \u001b[0;34m\"Please check that you entered the correct \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    272\u001b[0m                       \"path.\")%filename)\n",
            "\u001b[0;31mOSError\u001b[0m: MoviePy error: the file test_videos/challenge.mp4 could not be found!\nPlease check that you entered the correct path."
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "<video width=\"960\" height=\"540\" controls>\n",
              "  <source src=\"output_videos\\challenge_output.mp4\">\n",
              "</video>\n"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "pfpNT8QFYFEo"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}